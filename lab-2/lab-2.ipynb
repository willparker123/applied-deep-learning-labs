{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab-2.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXpasdqFo3Jg"
      },
      "source": [
        "# **AlexNet CNN on CIFAR-10 dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1EjufH1omSl"
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "import time\n",
        "from multiprocessing import cpu_count\n",
        "from typing import Union, NamedTuple\n",
        "\n",
        "import torch\n",
        "import torch.backends.cudnn\n",
        "import numpy as np\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "import torchvision.datasets\n",
        "from torch.optim.optimizer import Optimizer\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import transforms\n",
        "\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "parser = argparse.ArgumentParser(\n",
        "    description=\"Train a simple CNN on CIFAR-10\",\n",
        "    formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n",
        ")\n",
        "default_dataset_dir = Path.home() / \".cache\" / \"torch\" / \"datasets\"\n",
        "parser.add_argument(\"--dataset-root\", default=default_dataset_dir)\n",
        "parser.add_argument(\"--log-dir\", default=Path(\"logs\"), type=Path)\n",
        "parser.add_argument(\"--learning-rate\", default=1e-2, type=float, help=\"Learning rate\")\n",
        "parser.add_argument(\n",
        "    \"--batch-size\",\n",
        "    default=128,\n",
        "    type=int,\n",
        "    help=\"Number of images within each mini-batch\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--epochs\",\n",
        "    default=20,\n",
        "    type=int,\n",
        "    help=\"Number of epochs (passes through the entire dataset) to train for\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--val-frequency\",\n",
        "    default=2,\n",
        "    type=int,\n",
        "    help=\"How frequently to test the model on the validation set in number of epochs\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--log-frequency\",\n",
        "    default=10,\n",
        "    type=int,\n",
        "    help=\"How frequently to save logs to tensorboard in number of steps\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--print-frequency\",\n",
        "    default=10,\n",
        "    type=int,\n",
        "    help=\"How frequently to print progress to the command line in number of steps\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"-j\",\n",
        "    \"--worker-count\",\n",
        "    default=cpu_count(),\n",
        "    type=int,\n",
        "    help=\"Number of worker processes used to load data.\",\n",
        ")\n",
        "\n",
        "\n",
        "class ImageShape(NamedTuple):\n",
        "    height: int\n",
        "    width: int\n",
        "    channels: int\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = torch.device(\"cuda\")\n",
        "else:\n",
        "    DEVICE = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    transform = transforms.ToTensor()\n",
        "    args.dataset_root.mkdir(parents=True, exist_ok=True)\n",
        "    train_dataset = torchvision.datasets.CIFAR10(\n",
        "        args.dataset_root, train=True, download=True, transform=transform\n",
        "    )\n",
        "    test_dataset = torchvision.datasets.CIFAR10(\n",
        "        args.dataset_root, train=False, download=False, transform=transform\n",
        "    )\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        shuffle=True,\n",
        "        batch_size=args.batch_size,\n",
        "        pin_memory=True,\n",
        "        num_workers=args.worker_count,\n",
        "    )\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        shuffle=False,\n",
        "        batch_size=args.batch_size,\n",
        "        num_workers=args.worker_count,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "    model = CNN(height=32, width=32, channels=3, class_count=10)\n",
        "\n",
        "    loss_f = nn.CrossEntropyLoss()\n",
        "    criterion = loss_f  #lambda logits, labels: torch.tensor(0)\n",
        "    ## TASK 11: Define the optimizer\n",
        "    optimizer = optim.SGD(model.parameters(), lr=args.learning_rate)\n",
        "\n",
        "    log_dir = get_summary_writer_log_dir(args)\n",
        "    print(f\"Writing logs to {log_dir}\")\n",
        "    summary_writer = SummaryWriter(\n",
        "            str(log_dir),\n",
        "            flush_secs=5\n",
        "    )\n",
        "    trainer = Trainer(\n",
        "        model, train_loader, test_loader, criterion, optimizer, summary_writer, DEVICE\n",
        "    )\n",
        "\n",
        "    trainer.train(\n",
        "        args.epochs,\n",
        "        args.val_frequency,\n",
        "        print_frequency=args.print_frequency,\n",
        "        log_frequency=args.log_frequency,\n",
        "    )\n",
        "    \n",
        "    summary_writer.close()\n",
        "\n",
        "\n",
        "\n",
        "class ImageShape(NamedTuple):\n",
        "    height: int\n",
        "    width: int\n",
        "    channels: int\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, height: int, width: int, channels: int, class_count: int):\n",
        "        super().__init__()\n",
        "        self.input_shape = ImageShape(height=height, width=width, channels=channels)\n",
        "        self.class_count = class_count\n",
        "\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels=self.input_shape.channels,\n",
        "            out_channels=32,\n",
        "            kernel_size=(5, 5),\n",
        "            padding=(2, 2),\n",
        "        )\n",
        "        self.initialise_layer(self.conv1)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            in_channels=self.input_shape.channels,\n",
        "            out_channels=64,\n",
        "            kernel_size=(5, 5),\n",
        "            padding=(2, 2),\n",
        "        )\n",
        "        self.initialise_layer(self.conv2)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
        "        self.fc1 = nn.Linear(4096, 1024)\n",
        "        self.initialise_layer(self.fc1)\n",
        "        self.fc2 = nn.Linear(1024, 10)\n",
        "        self.initialise_layer(self.fc2)\n",
        "\n",
        "    def forward(self, images: torch.Tensor) -> torch.Tensor:\n",
        "        x = F.relu(self.conv1(images))\n",
        "        x = self.pool1(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool2(x)\n",
        "        torch.flatten(x, start_dim=1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def initialise_layer(layer):\n",
        "        if hasattr(layer, \"bias\"):\n",
        "            nn.init.zeros_(layer.bias)\n",
        "        if hasattr(layer, \"weight\"):\n",
        "            nn.init.kaiming_normal_(layer.weight)\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        train_loader: DataLoader,\n",
        "        val_loader: DataLoader,\n",
        "        criterion: nn.Module,\n",
        "        optimizer: Optimizer,\n",
        "        summary_writer: SummaryWriter,\n",
        "        device: torch.device,\n",
        "    ):\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.summary_writer = summary_writer\n",
        "        self.step = 0\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        epochs: int,\n",
        "        val_frequency: int,\n",
        "        print_frequency: int = 20,\n",
        "        log_frequency: int = 5,\n",
        "        start_epoch: int = 0\n",
        "    ):\n",
        "        self.model.train()\n",
        "        for epoch in range(start_epoch, epochs):\n",
        "            self.model.train()\n",
        "            data_load_start_time = time.time()\n",
        "            for batch, labels in self.train_loader:\n",
        "                batch = batch.to(self.device)\n",
        "                labels = labels.to(self.device)\n",
        "                data_load_end_time = time.time()\n",
        "\n",
        "\n",
        "\n",
        "                logits = self.model.forward(batch)\n",
        "                loss = self.criterion(logits, labels)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    preds = logits.argmax(-1)\n",
        "                    accuracy = compute_accuracy(labels, preds)\n",
        "\n",
        "                data_load_time = data_load_end_time - data_load_start_time\n",
        "                step_time = time.time() - data_load_end_time\n",
        "                if ((self.step + 1) % log_frequency) == 0:\n",
        "                    self.log_metrics(epoch, accuracy, loss, data_load_time, step_time)\n",
        "                if ((self.step + 1) % print_frequency) == 0:\n",
        "                    self.print_metrics(epoch, accuracy, loss, data_load_time, step_time)\n",
        "\n",
        "                self.step += 1\n",
        "                data_load_start_time = time.time()\n",
        "\n",
        "            self.summary_writer.add_scalar(\"epoch\", epoch, self.step)\n",
        "            if ((epoch + 1) % val_frequency) == 0:\n",
        "                self.validate()\n",
        "                # self.validate() will put the model in validation mode,\n",
        "                # so we have to switch back to train mode afterwards\n",
        "                self.model.train()\n",
        "\n",
        "    def print_metrics(self, epoch, accuracy, loss, data_load_time, step_time):\n",
        "        epoch_step = self.step % len(self.train_loader)\n",
        "        print(\n",
        "                f\"epoch: [{epoch}], \"\n",
        "                f\"step: [{epoch_step}/{len(self.train_loader)}], \"\n",
        "                f\"batch loss: {loss:.5f}, \"\n",
        "                f\"batch accuracy: {accuracy * 100:2.2f}, \"\n",
        "                f\"data load time: \"\n",
        "                f\"{data_load_time:.5f}, \"\n",
        "                f\"step time: {step_time:.5f}\"\n",
        "        )\n",
        "\n",
        "    def log_metrics(self, epoch, accuracy, loss, data_load_time, step_time):\n",
        "        self.summary_writer.add_scalar(\"epoch\", epoch, self.step)\n",
        "        self.summary_writer.add_scalars(\n",
        "                \"accuracy\",\n",
        "                {\"train\": accuracy},\n",
        "                self.step\n",
        "        )\n",
        "        self.summary_writer.add_scalars(\n",
        "                \"loss\",\n",
        "                {\"train\": float(loss.item())},\n",
        "                self.step\n",
        "        )\n",
        "        self.summary_writer.add_scalar(\n",
        "                \"time/data\", data_load_time, self.step\n",
        "        )\n",
        "        self.summary_writer.add_scalar(\n",
        "                \"time/data\", step_time, self.step\n",
        "        )\n",
        "\n",
        "    def validate(self):\n",
        "        results = {\"preds\": [], \"labels\": []}\n",
        "        total_loss = 0\n",
        "        self.model.eval()\n",
        "\n",
        "        # No need to track gradients for validation, we're not optimizing.\n",
        "        with torch.no_grad():\n",
        "            for batch, labels in self.val_loader:\n",
        "                batch = batch.to(self.device)\n",
        "                labels = labels.to(self.device)\n",
        "                logits = self.model(batch)\n",
        "                loss = self.criterion(logits, labels)\n",
        "                total_loss += loss.item()\n",
        "                preds = logits.argmax(dim=-1).cpu().numpy()\n",
        "                results[\"preds\"].extend(list(preds))\n",
        "                results[\"labels\"].extend(list(labels.cpu().numpy()))\n",
        "\n",
        "        accuracy = compute_accuracy(\n",
        "            np.array(results[\"labels\"]), np.array(results[\"preds\"])\n",
        "        )\n",
        "        average_loss = total_loss / len(self.val_loader)\n",
        "\n",
        "        self.summary_writer.add_scalars(\n",
        "                \"accuracy\",\n",
        "                {\"test\": accuracy},\n",
        "                self.step\n",
        "        )\n",
        "        self.summary_writer.add_scalars(\n",
        "                \"loss\",\n",
        "                {\"test\": average_loss},\n",
        "                self.step\n",
        "        )\n",
        "        print(f\"validation loss: {average_loss:.5f}, accuracy: {accuracy * 100:2.2f}\")\n",
        "\n",
        "\n",
        "def compute_accuracy(\n",
        "    labels: Union[torch.Tensor, np.ndarray], preds: Union[torch.Tensor, np.ndarray]\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        labels: ``(batch_size, class_count)`` tensor or array containing example labels\n",
        "        preds: ``(batch_size, class_count)`` tensor or array containing model prediction\n",
        "    \"\"\"\n",
        "    assert len(labels) == len(preds)\n",
        "    return float((labels == preds).sum()) / len(labels)\n",
        "\n",
        "\n",
        "def get_summary_writer_log_dir(args: argparse.Namespace) -> str:\n",
        "    \"\"\"Get a unique directory that hasn't been logged to before for use with a TB\n",
        "    SummaryWriter.\n",
        "\n",
        "    Args:\n",
        "        args: CLI Arguments\n",
        "\n",
        "    Returns:\n",
        "        Subdirectory of log_dir with unique subdirectory name to prevent multiple runs\n",
        "        from getting logged to the same TB log directory (which you can't easily\n",
        "        untangle in TB).\n",
        "    \"\"\"\n",
        "    tb_log_dir_prefix = f'CNN_bs={args.batch_size}_lr={args.learning_rate}_run_'\n",
        "    i = 0\n",
        "    while i < 1000:\n",
        "        tb_log_dir = args.log_dir / (tb_log_dir_prefix + str(i))\n",
        "        if not tb_log_dir.exists():\n",
        "            return str(tb_log_dir)\n",
        "        i += 1\n",
        "    return str(tb_log_dir)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(parser.parse_args())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}