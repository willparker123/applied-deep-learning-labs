{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMOJGrgrkW1NCIHWeeOlqBH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/willparker123/applied-deep-learning-labs/blob/master/lab-1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXpasdqFo3Jg"
      },
      "source": [
        "# **Pytorch and Numpy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1EjufH1omSl"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "array_np = np.array([[1, 2, 3],\n",
        "                     [4, 5, 6]])\n",
        "array_pytorch = torch.tensor([[1, 2, 3],\n",
        "                              [4, 5, 6]])\n",
        "x = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
        "y = torch.tensor([4, 5, 6], dtype=torch.float32)\n",
        "\n",
        "print(array_np)\n",
        "print(array_pytorch)\n",
        "print(x.shape, x.dim())\n",
        "#elementwise; x+y, x/y, x*y\n",
        "#returns tensor; torch.dot(x, y), x.mean(), x.std(), x.max(), x.argmax()\n",
        "#to get item from tensor; torch.dot(x, y).item()\n",
        "#to remove dimensions of size 1; x.squeeze()\n",
        "#to add back dimensions of size 1 plus a new size-1 dimension at index 4; x.unsqueeze(dim=4)\n",
        "print(torch.arange(0, 9).reshape((3, 3)))\n",
        "print(x @ y)\n",
        "print(torch.randn((2, 3)))\n",
        "print(x.reshape((3, 1)))\n",
        "#y = x.reshape(3, 1), y[0,0]=1 will change x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6pWi6R_IPR6"
      },
      "source": [
        "# **Fully-Connected Network**\n",
        "## **Data Processing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsUR-3nPoxtt"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "iris = datasets.load_iris()  # datasets are stored in a dictionary containing an array of features and targets\n",
        "print(iris.keys())\n",
        "print(f\"Data (first 15): {iris['data'][:15]}, Features: {iris['feature_names']}\")\n",
        "print(f\"Classes: {np.unique(iris['target'])}, Class Names: {iris['target_names']}\")\n",
        "\n",
        "#show pairplot of features\n",
        "features_df = pd.DataFrame(\n",
        "    iris['data'],\n",
        "    columns=iris['feature_names']\n",
        ")\n",
        "features_df['label'] = iris['target_names'][iris['target']]\n",
        "sns.pairplot(features_df, hue='label')\n",
        "\n",
        "#normalisation\n",
        "preprocessed_features = (iris['data'] - iris['data'].mean(axis=0)) / iris['data'].std(axis=0)\n",
        "\n",
        "#shuffling - train/test set\n",
        "labels = iris['target']\n",
        "# train_test_split takes care of the shuffling and splitting process\n",
        "train_features, test_features, train_labels, test_labels = train_test_split(preprocessed_features, labels, test_size=1/3)\n",
        "#convert to tensors\n",
        "features = {\n",
        "    'train': torch.tensor(train_features, dtype=torch.float32),\n",
        "    'test': torch.tensor(test_features, dtype=torch.float32),\n",
        "}\n",
        "labels = {\n",
        "    'train': torch.tensor(train_labels, dtype=torch.long),\n",
        "    'test': torch.tensor(test_labels, dtype=torch.long),\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUMp99FlKEtA"
      },
      "source": [
        "## **MLP**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6ipBXzQKJI8"
      },
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from typing import Callable\n",
        "from torch import optim\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_size: int,\n",
        "                 hidden_layer_size: int,\n",
        "                 output_size: int,\n",
        "                 activation_fn: Callable[[torch.Tensor], torch.Tensor] = F.relu):\n",
        "        super().__init__()\n",
        "        self.l1 = nn.Linear(input_size, hidden_layer_size)\n",
        "        self.l2 = nn.Linear(hidden_layer_size, output_size)\n",
        "        self.activation_fn = activation_fn\n",
        "        \n",
        "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.l1(inputs)\n",
        "        x = self.activation_fn(x)\n",
        "        x = self.l2(x)\n",
        "        return x\n",
        "\n",
        "feature_count = 4\n",
        "hidden_layer_size = 100\n",
        "class_count = 3\n",
        "model = MLP(feature_count, hidden_layer_size, class_count)\n",
        "\n",
        "#compute logits - values for each class and sample\n",
        "#logits = model.forward(features['train'])\n",
        "#logits.shape\n",
        "\n",
        "#initialise cross-entropy loss function; combines cross-entropy and softmax\n",
        "loss_f = nn.CrossEntropyLoss()\n",
        "\n",
        "#compute loss\n",
        "#loss = loss_f(logits, labels['train'])\n",
        "##use loss by backpropagating through network to get gradients\n",
        "#loss.backward()\n",
        "\n",
        "def accuracy(probs: torch.FloatTensor, targets: torch.LongTensor) -> float:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        probs: A float32 tensor of shape ``(batch_size, class_count)`` where each value \n",
        "            at index ``i`` in a row represents the score of class ``i``.\n",
        "        targets: A long tensor of shape ``(batch_size,)`` containing the batch examples'\n",
        "            labels.\n",
        "    \"\"\"\n",
        "    count = 0\n",
        "    amx = probs.argmax(dim=1)\n",
        "    for x in range(len(probs)):\n",
        "      if targets[x]==amx[x]:\n",
        "        count+=1\n",
        "    return count / targets.shape[0]\n",
        "    ## First work out which class has been predicted for each data sample. Hint: use argmax\n",
        "    ## Second count how many of these are correctly predicted\n",
        "    ## Finally return the accuracy, i.e. the percentage of samples correctly predicted\n",
        "\n",
        "def check_accuracy(probs: torch.FloatTensor,\n",
        "                   labels: torch.LongTensor,\n",
        "                   expected_accuracy: float):\n",
        "    actual_accuracy = float(accuracy(probs, labels))\n",
        "    assert actual_accuracy == expected_accuracy, f\"Expected accuracy to be {expected_accuracy} but was {actual_accuracy}\"\n",
        "\n",
        "check_accuracy(torch.tensor([[0, 1],\n",
        "                             [0, 1],\n",
        "                             [0, 1],\n",
        "                             [0, 1],\n",
        "                             [0, 1]]),\n",
        "               torch.ones(5, dtype=torch.long),\n",
        "               1.0)\n",
        "check_accuracy(torch.tensor([[1, 0],\n",
        "                             [0, 1],\n",
        "                             [0, 1],\n",
        "                             [0, 1],\n",
        "                             [0, 1]]),\n",
        "               torch.ones(5, dtype=torch.long),\n",
        "               0.8)\n",
        "check_accuracy(torch.tensor([[1, 0],\n",
        "                             [1, 0],\n",
        "                             [0, 1],\n",
        "                             [0, 1],\n",
        "                             [0, 1]]),\n",
        "               torch.ones(5, dtype=torch.long),\n",
        "               0.6)\n",
        "check_accuracy(torch.tensor([[1, 0],\n",
        "                             [1, 0],\n",
        "                             [1, 0],\n",
        "                             [1, 0],\n",
        "                             [1, 0]]),\n",
        "               torch.ones(5, dtype=torch.long),\n",
        "               0.0)\n",
        "print(\"All test cases passed\")\n",
        "\n",
        "\n",
        "\n",
        "# The optimizer we'll use to update the model parameters\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.05)\n",
        "# Now we define the loss function.\n",
        "criterion = loss_f\n",
        "# Now we iterate over the dataset a number of times. Each iteration of the entire dataset \n",
        "# is called an epoch.\n",
        "for epoch in range(0, 100):\n",
        "    # We compute the forward pass of the network\n",
        "    _logits = model.forward(features['train'])\n",
        "    # Then the value of loss function \n",
        "    _loss = criterion(_logits,  labels['train'])\n",
        "    # How well the network does on the batch is an indication of how well training is \n",
        "    # progressing\n",
        "    print(\"epoch: {} train accuracy: {:2.2f}, loss: {:5.5f}\".format(\n",
        "        epoch,\n",
        "        accuracy(_logits, labels['train']) * 100,\n",
        "        _loss.item()\n",
        "    ))\n",
        "    # Now we compute the backward pass, which populates the `.grad` attributes of the parameters\n",
        "    _loss.backward()\n",
        "    # Now we update the model parameters using those gradients\n",
        "    optimizer.step()\n",
        "    # Now we need to zero out the `.grad` buffers as otherwise on the next backward pass we'll add the \n",
        "    # new gradients to the old ones.\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "# Finally we can test our model on the test set and get an unbiased estimate of its performance.    \n",
        "_logits = model.forward(features['test'])    \n",
        "test_accuracy = accuracy(_logits, labels['test']) * 100\n",
        "print(\"test accuracy: {:2.2f}\".format(test_accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPR7buPqq2oL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}